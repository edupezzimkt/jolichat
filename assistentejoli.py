import openai 
import fitz  # PyMuPDF
import json
import os
import tiktoken
import streamlit as st

# Carregar a chave da OpenAI dos secrets do Streamlit
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Fun√ß√£o para extrair texto do PDF
def extrair_texto_pdf(caminho_pdf):
    documento = fitz.open(caminho_pdf)
    texto_completo = ""
    for pagina in documento:
        texto_completo += pagina.get_text()
    return texto_completo

# Fun√ß√£o para carregar cache de um arquivo JSON
def carregar_cache(nome_arquivo):
    if os.path.exists(nome_arquivo):
        try:
            with open(nome_arquivo, 'r', encoding='utf-8') as arquivo:
                return json.load(arquivo)
        except json.JSONDecodeError:
            print(f"Erro ao carregar o cache de {nome_arquivo}. O arquivo est√° corrompido.")
    return {}

# Fun√ß√£o para salvar cache em um arquivo JSON
def salvar_cache(cache, arquivo):
    cache_convertido = {str(k): v for k, v in cache.items()}
    with open(arquivo, 'w', encoding='utf-8') as f:
        json.dump(cache_convertido, f, ensure_ascii=False, indent=4)

# Inicializa o cache
cache_arquivo = 'cache.json'
cache = carregar_cache(cache_arquivo)

# Carregar texto extra√≠do dos PDFs com cache
def carregar_ou_processar_pdf(caminho_pdf, cache):
    if caminho_pdf in cache:
        return cache[caminho_pdf]
    texto_pdf = extrair_texto_pdf(caminho_pdf)
    cache[caminho_pdf] = texto_pdf
    salvar_cache(cache, cache_arquivo)
    return texto_pdf

# Fun√ß√£o para obter todos os PDFs em uma pasta
def obter_caminhos_pdfs(pasta):
    return [os.path.join(pasta, f) for f in os.listdir(pasta) if f.endswith('.pdf')]

# Especificar a pasta contendo os PDFs
pasta_pdfs = 'arquivos'

# Obter todos os caminhos dos PDFs na pasta
caminhos_pdfs = obter_caminhos_pdfs(pasta_pdfs)

# Carregar texto extra√≠do dos PDFs
textos_pdfs = [carregar_ou_processar_pdf(caminho, cache) for caminho in caminhos_pdfs]
texto_completo_pdfs = "\n".join(textos_pdfs)

# Fun√ß√£o para limitar o hist√≥rico de mensagens
def limitar_historico(mensagens, max_mensagens=5):
    if len(mensagens) > max_mensagens:
        return mensagens[-max_mensagens:]
    return mensagens

# Prompt espec√≠fico
prompt = """Voc√™ √© uma assistente bem humorada especialista em turismo e um sommelier internacional. 
Seu nome √© Joli e vai responder de forma informal e usar os PDFs que est√£o na pasta 'arquivos' 
e responder√° de forma curta pegando informa√ß√µes dos passeios e tirando as d√∫vidas dos turistas.
Lembre-se que o cliente pode pedir informa√ß√µes sobre produtos e sobre os passeios (tours, visita√ß√£o), 
quando a convera for sobre produtos direcionar para informa√ß√µes dos produtos e contatos e n√£o falar dos passeios, 
quando for sobre turismo e passeio, visita√ß√£o, conhecer a vin√≠cola usar as informa√ß√µes dos passeios."""

# Fun√ß√£o para gera√ß√£o de texto usando o contexto e o prompt
def geracao_texto(pergunta_usuario, contexto, prompt):
    chave_cache = (pergunta_usuario,)

    if chave_cache in cache:
        resposta_cache = cache[chave_cache]
        return resposta_cache
    
    # Montar a mensagem com o prompt e contexto
    mensagens = [
        {"role": "system", "content": prompt + "\n\n" + contexto},
        {"role": "user", "content": pergunta_usuario}
    ]
    
    resposta = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", # , gpt-4o-mini
        messages=mensagens,
        temperature=0.5,  # Um pouco de criatividade no tom
        max_tokens=500,   # Limite de tokens para respostas mais curtas
        top_p=1,
        frequency_penalty=0.2,
        presence_penalty=0.6,
    )

    texto_resposta = resposta['choices'][0]['message']['content']
    
    # Adicionar a resposta ao cache
    cache[chave_cache] = texto_resposta
    salvar_cache(cache, cache_arquivo)
    
    return texto_resposta

# Streamlit interface

# CSS para esconder o bot√£o flutuante no mobile usando seletor mais espec√≠fico
hide_streamlit_style = """
    <style>
    /* Esconde a barra de ferramentas no desktop */
    MainMenu {visibility: hidden;}
    
    /* Esconde o rodap√© */
    footer {visibility: hidden;}
    
    /* Esconde o cabe√ßalho */
    header {visibility: hidden;}

    /* Esconde o bot√£o flutuante no mobile com seletor CSS baseado no XPath */
    [data-testid="stSidebarNav"] {display: none;} /* Este deve funcionar para a vers√£o mais recente do Streamlit */
    
    /* Sele√ß√£o direta com base no XPath convertido */
    div[role="button"] > svg {display: none;} /* Seletor CSS para o bot√£o flutuante do menu */
    </style>
    """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

st.title("Bem-vindo ao chat da Jolimontüç∑")

# Inicializar mensagens na sess√£o se ainda n√£o existirem
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Ol√°, estou aqui para responder sobre produtos e passeios.üçá"}]

# Exibir as mensagens anteriores do chat
for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

# Input de pergunta do usu√°rio
if pergunta_usuario := st.chat_input("Fa√ßa sua pergunta"):
    # Adicionar a mensagem do usu√°rio ao estado da sess√£o
    st.session_state["messages"].append({"role": "user", "content": pergunta_usuario})
    with st.chat_message("user"):
        st.write(pergunta_usuario)

    # Gerar resposta usando os textos dos PDFs carregados e o prompt
    resposta_assistente = geracao_texto(pergunta_usuario, texto_completo_pdfs, prompt)
    
    # Adicionar a resposta ao estado da sess√£o
    st.session_state["messages"].append({"role": "assistant", "content": resposta_assistente})
    
    # Exibir a resposta gerada
    with st.chat_message("assistant"):
        st.write(resposta_assistente)
